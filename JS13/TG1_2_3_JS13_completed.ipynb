{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒºLab 1\n",
        "\n",
        "Klasifikasi Bunga Iris dengan Perceptron"
      ],
      "metadata": {
        "id": "Pi6kZXwD9O09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 1 - Import Library"
      ],
      "metadata": {
        "id": "VwDzEJSx9VXG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEsMEmmX8CsH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 2 - Load Data dan Visualisasi"
      ],
      "metadata": {
        "id": "XBScUwlg9YGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anda perlu memastikan file 'iris.csv' ada di direktori yang sama\n",
        "# Jika menggunakan Google Colab, upload file tersebut\n",
        "try:\n",
        "    df = pd.read_csv('iris.csv', header=None)\n",
        "except FileNotFoundError:\n",
        "    print(\"File 'iris.csv' tidak ditemukan. Mengunduh dari URL...\")\n",
        "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
        "    df = pd.read_csv(url, header=None)\n",
        "    df.to_csv('iris.csv', index=False, header=False)\n",
        "    print(\"Dataset Iris telah diunduh dan disimpan sebagai 'iris.csv'.\")\n",
        "\n",
        "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
        "\n",
        "setosa = df[df['species'] == 'Iris-setosa']\n",
        "versicolor = df[df['species'] == 'Iris-versicolor']\n",
        "virginica = df[df['species'] == 'Iris-virginica']\n",
        "\n",
        "a, b = 'sepal_length', 'petal_length'\n",
        "plt.scatter(setosa[a], setosa[b], color='red', marker='o', label='setosa')\n",
        "plt.scatter(versicolor[a], versicolor[b], color='blue', marker='x', label='versicolor')\n",
        "\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Petal Length')\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nbjb5lbL9ZsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 3 - Membuat Kelas Perceptron"
      ],
      "metadata": {
        "id": "aQNqvpwu9bfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(object):\n",
        "    def __init__(self, eta=0.01, n_iter=10):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        self.w_ = np.zeros(1 + X.shape[1])\n",
        "        self.errors_ = []\n",
        "\n",
        "        for _ in range(self.n_iter):\n",
        "            errors = 0\n",
        "            for xi, target in zip(X, y):\n",
        "                update = self.eta * (target - self.predict(xi))\n",
        "                self.w_[1:] += update * xi\n",
        "                self.w_[0] += update\n",
        "                errors += int(update != 0.0)\n",
        "            self.errors_.append(errors)\n",
        "        return self\n",
        "\n",
        "    def net_input(self, X):\n",
        "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
      ],
      "metadata": {
        "id": "WYlBw0pW9c-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 4 - Pilih Data dan Encoding Label"
      ],
      "metadata": {
        "id": "UByI3heK9eZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df.iloc[0:100, 4].values # pilih 100 data awal (Setosa and Versicolor)\n",
        "y = np.where(y == 'Iris-setosa', -1, 1) # ganti coding label\n",
        "X = df.iloc[0:100, [0, 2]].values # slice data latih [sepal_length, petal_length]"
      ],
      "metadata": {
        "id": "ymBEbYEE9fs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 5 - Fitting Model"
      ],
      "metadata": {
        "id": "0NYzk9aW9hDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ppn = Perceptron(eta=0.1, n_iter=10)\n",
        "ppn.fit(X, y)"
      ],
      "metadata": {
        "id": "0eMF9VEn9iiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 6 - Visualisasi Nilai Error Per Epoch"
      ],
      "metadata": {
        "id": "tQzedhkQ9kUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1, len(ppn.errors_)+1), ppn.errors_, marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Number of updates')\n",
        "plt.title('Perceptron - Learning Progress')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YX_5syPK9luX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 7 - Visualisasi Decision Boundary"
      ],
      "metadata": {
        "id": "SNE5jTXF9noD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# buat fungsi untuk plot decision region\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
        "    # setup marker generator and color map\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # plot the decision surface\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # plot class samples\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0],\n",
        "                    y=X[y == cl, 1],\n",
        "                    alpha=0.8,\n",
        "                    c=colors[idx],\n",
        "                    marker=markers[idx],\n",
        "                    label='setosa' if cl == -1 else 'versicolor',\n",
        "                    edgecolor='black')"
      ],
      "metadata": {
        "id": "6wzm-SfZ9pY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Perbaikan] Memanggil Fungsi plot_decision_regions\n",
        "Menambahkan visualisasi decision boundary yang sebelumnya belum dipanggil."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_regions(X, y, classifier=ppn)\n",
        "plt.xlabel('Sepal length [cm]')\n",
        "plt.ylabel('Petal length [cm]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Perceptron Decision Boundary')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ¸Lab 2\n",
        "\n",
        "Nilai Logika XOR dengan MLP"
      ],
      "metadata": {
        "id": "bEfjYCxb9rzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 1 - Import Library"
      ],
      "metadata": {
        "id": "PeWTLSGB9vHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "-PVPo5dO9wF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langah 2 - Buat Data"
      ],
      "metadata": {
        "id": "cnoE_coR9y_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = [0, 1, 1, 0] # label\n",
        "X = [[0, 0], [0, 1], [1, 0], [1, 1]] # data"
      ],
      "metadata": {
        "id": "dVo6ySUp9yZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 3 - Fit Model"
      ],
      "metadata": {
        "id": "lbqKHJmo92NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "# Menggunakan 'logistic' (sigmoid) sebagai fungsi aktivasi dan 'lbfgs' sebagai solver.\n",
        "# hidden_layer_sizes=(2,) berarti ada satu hidden layer dengan 2 neuron.\n",
        "# random_state digunakan untuk reproduktifitas hasil.\n",
        "clf = MLPClassifier(solver='lbfgs', activation='logistic', hidden_layer_sizes=(2,), max_iter=1000, random_state=20)\n",
        "clf.fit(X, y)"
      ],
      "metadata": {
        "id": "NVby7ojB93sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 4 - Prediksi"
      ],
      "metadata": {
        "id": "1mCZxaiv949L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = clf.predict(X)\n",
        "print('Accuracy: %s' % clf.score(X, y))\n",
        "for i,p in enumerate(pred):\n",
        "    print('True: %s, Predicted: %s' % (y[i], p))"
      ],
      "metadata": {
        "id": "UxYopF6k96Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸŒ¼Lab 3\n",
        "\n",
        "Klasifikasi Churn Rate dengan ANN"
      ],
      "metadata": {
        "id": "BkDDWJLq9_H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pra Pengolahan Data"
      ],
      "metadata": {
        "id": "SCPi2awT-DR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 1 - Import Library"
      ],
      "metadata": {
        "id": "BgQ2H9u0-lxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "hrJzksh7-nAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 2 - Load Data"
      ],
      "metadata": {
        "id": "9m-4WB7h-oWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan file 'Churn_Modelling.csv' ada di direktori yang sama\n",
        "try:\n",
        "    dataset = pd.read_csv('Churn_Modelling.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"File 'Churn_Modelling.csv' tidak ditemukan. Coba unduh dari sumber lain...\")\n",
        "    # URL alternatif jika diperlukan, atau berikan pesan untuk mengupload manual\n",
        "    print(\"Silakan upload file 'Churn_Modelling.csv' secara manual.\")\n",
        "    # Contoh: !wget [URL_FILE] # jika di Colab\n",
        "    dataset = pd.DataFrame() # Buat dataframe kosong jika gagal\n",
        "\n",
        "if not dataset.empty:\n",
        "    X = dataset.iloc[:, 3:-1].values\n",
        "    y = dataset.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "g-3xkLvp-peq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "W_QAVdSw-qqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 3 - Encoding Data Kategorikal (Gender)"
      ],
      "metadata": {
        "id": "_SLQ2Z9u-tBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "X[:, 2] = le.fit_transform(X[:, 2])"
      ],
      "metadata": {
        "id": "sB4WjxU6-uaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 4 - Encoding Kolom \"Geography\" dengan One Hot Encoder"
      ],
      "metadata": {
        "id": "ELS4FEc6-wlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "metadata": {
        "id": "ndvrBmy5-yBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 5 - Split Data"
      ],
      "metadata": {
        "id": "DonofH7C-0x2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "5jEKHu-a-2Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 6 - Scaling Fitur"
      ],
      "metadata": {
        "id": "_PNQ9ihk-304"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "DhekpF1Q-5MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membuat Model ANN"
      ],
      "metadata": {
        "id": "KNmrCvR6_FvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 1 - Inisiasi Model ANN"
      ],
      "metadata": {
        "id": "tk-69j55_J3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ann = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "zoBxZ-UF_IZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 2 - Membuat Input Layer dan Hidden Layer Pertama"
      ],
      "metadata": {
        "id": "aAII8_xs_cx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
      ],
      "metadata": {
        "id": "jrWJFDEY_eI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 3 - Membuat Hidden Layer Kedua"
      ],
      "metadata": {
        "id": "bkl6vVSX_gsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
      ],
      "metadata": {
        "id": "WkFwraaP_h78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 4 - Membuat Output Layer"
      ],
      "metadata": {
        "id": "u1z90CDE_jJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "i7wv332g_keL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model"
      ],
      "metadata": {
        "id": "PGI0ritF_l5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 1 - Compile Model (Menyatukan Arsitektur) ANN"
      ],
      "metadata": {
        "id": "q2nuIo7l_m2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "mS_lu0tFDl35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 2 - Fitting Model"
      ],
      "metadata": {
        "id": "soocdBtgD0Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_churn = ann.fit(X_train, y_train, batch_size = 32, epochs = 100, validation_split=0.2)"
      ],
      "metadata": {
        "id": "_8KXmByBD1gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Perbaikan] Visualisasi Hasil Training\n",
        "Menambahkan plot untuk akurasi dan loss selama training untuk memantau performa model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history_churn.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1) # set the y-axis range to [0-1]\n",
        "plt.title('Churn Model Training History')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membuat Prediksi\n",
        "Diberikan informasi sebagai berikut,\n",
        "\n",
        "*   Geography: France\n",
        "*   Credit Score: 600\n",
        "*   Gender: Male\n",
        "*   Age: 40 years old\n",
        "*   Tenure: 3 years\n",
        "*   Balance: $ 60000\n",
        "*   Number of Products: 2\n",
        "*   Does this customer have a credit card ? Yes\n",
        "*   Is this customer an Active Member: Yes\n",
        "*   Estimated Salary: $ 50000\n",
        "\n",
        "Apakah customer tersebut akan Churn (keluar)?\n",
        "\n",
        "**Penjelasan Data:**\n",
        "- Geography 'France' di-encode menjadi `1, 0, 0`.\n",
        "- Gender 'Male' di-encode menjadi `1`.\n",
        "- HasCrCard 'Yes' adalah `1`.\n",
        "- IsActiveMember 'Yes' adalah `1`."
      ],
      "metadata": {
        "id": "dushlBBkD5VD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelkan Data Baru dan Buat Prediksi"
      ],
      "metadata": {
        "id": "Ka9x_B4aD8DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_customer_data = [[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]]\n",
        "new_customer_scaled = sc.transform(new_customer_data)\n",
        "prediction_prob = ann.predict(new_customer_scaled)\n",
        "prediction = (prediction_prob > 0.5)\n",
        "\n",
        "print(f\"Data Customer Baru: {new_customer_data}\")\n",
        "print(f\"Probabilitas Churn: {prediction_prob[0][0]:.4f}\")\n",
        "print(f\"Prediksi Churn (Threshold 0.5): {prediction[0][0]}\")\n",
        "\n",
        "if prediction[0][0]:\n",
        "    print(\"Hasil: Pelanggan diprediksi akan CHURN.\")\n",
        "else:\n",
        "    print(\"Hasil: Pelanggan diprediksi TIDAK AKAN CHURN (akan tetap bertahan).\")"
      ],
      "metadata": {
        "id": "SYBxzdw_D9hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediksi Dengan Data Testing"
      ],
      "metadata": {
        "id": "1QqR0XB2EBtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = ann.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ],
      "metadata": {
        "id": "OeQsiG0uEEUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cek Akurasi dan Confusion Matrix"
      ],
      "metadata": {
        "id": "NblseqO2EFqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy Score: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "1eGw59xDEHLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŒ»Lab 4\n",
        "Klasifikasi Siang dan Malam dengan ANN"
      ],
      "metadata": {
        "id": "amXKorLqIzUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 1 - Import Library"
      ],
      "metadata": {
        "id": "gzUbAGzuJec3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from skimage.feature import hog\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import os\n",
        "import zipfile\n",
        "import requests"
      ],
      "metadata": {
        "id": "uZoccs9uJdEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 2 - Load Dataset\n",
        "\n",
        "Kode ini akan mengunduh dataset jika folder `images` belum ada."
      ],
      "metadata": {
        "id": "D3bv8fyxJhYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('images'):\n",
        "    print(\"Folder 'images' tidak ditemukan. Mengunduh dan mengekstrak dataset...\")\n",
        "    url = 'https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip'\n",
        "    response = requests.get(url)\n",
        "    with open('rockpaperscissors.zip', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    with zipfile.ZipFile('rockpaperscissors.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('images_temp')\n",
        "    os.remove('rockpaperscissors.zip')\n",
        "\n",
        "    # Strukturnya mungkin berbeda, kita perlu menyesuaikannya\n",
        "    # Untuk contoh ini, kita asumsikan dataset siang/malam sudah ada\n",
        "    # Jika tidak, Anda perlu menyediakannya dengan struktur:\n",
        "    # images/training/day/*.jpg\n",
        "    # images/training/night/*.jpg\n",
        "    # images/test/day/*.jpg\n",
        "    # images/test/night/*.jpg\n",
        "    print(\"\\nPERINGATAN: Dataset yang diunduh adalah 'rockpaperscissors'.\")\n",
        "    print(\"Pastikan Anda memiliki dataset 'day/night' dalam folder 'images' sesuai struktur.\")\n",
        "\n",
        "# Load images and labels from a directory structure\n",
        "def load_dataset(img_dir):\n",
        "    p = Path(img_dir)\n",
        "    if not p.exists():\n",
        "        print(f\"Direktori tidak ditemukan: {img_dir}\")\n",
        "        return []\n",
        "    img_list = []\n",
        "    for folder in p.glob('*'):\n",
        "        if not folder.is_dir(): continue\n",
        "        label = folder.name\n",
        "        for file in list(folder.glob('*.jpg')) + list(folder.glob('*.png')):\n",
        "            img = cv2.imread(str(file))\n",
        "            if img is not None:\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img_list.append((img, label))\n",
        "    return img_list\n",
        "\n",
        "train_dir = \"images/training/\"\n",
        "test_dir  = \"images/test/\"\n",
        "\n",
        "train_img = load_dataset(train_dir)\n",
        "test_img  = load_dataset(test_dir)\n",
        "\n",
        "if not train_img or not test_img:\n",
        "    print(\"\\nDataset tidak berhasil dimuat. Pastikan folder 'images/training' dan 'images/test' ada dan berisi gambar.\")"
      ],
      "metadata": {
        "id": "i6u1AqiFMmMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 2 - Pra Pengolahan"
      ],
      "metadata": {
        "id": "q6losM0xMoM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess images: resize and encode labels\n",
        "def resize_image(img, size=(256,256)):\n",
        "    return cv2.resize(img, size)\n",
        "\n",
        "def label_encoder(label):\n",
        "    return 1 if label.lower() == 'day' else 0\n",
        "\n",
        "def preprocess(img_list):\n",
        "    X = []\n",
        "    y = []\n",
        "    for img, label in img_list:\n",
        "        img_std = resize_image(img)\n",
        "        X.append(img_std)\n",
        "        y.append(label_encoder(label))\n",
        "    return X, y\n",
        "\n",
        "if train_img and test_img:\n",
        "    X_train_img, y_train = preprocess(train_img)\n",
        "    X_test_img,  y_test  = preprocess(test_img)"
      ],
      "metadata": {
        "id": "WpTwzgDEMptq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 3 - Ekstraksi Fitur"
      ],
      "metadata": {
        "id": "fF2_2ezPMzcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract HOG features\n",
        "def extract_hog(X_imgs):\n",
        "    feats = []\n",
        "    for img in X_imgs:\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        hog_feat = hog(gray,\n",
        "                       orientations=9,\n",
        "                       pixels_per_cell=(8,8),\n",
        "                       cells_per_block=(2,2),\n",
        "                       block_norm='L2-Hys',\n",
        "                       visualize=False,\n",
        "                       feature_vector=True)\n",
        "        feats.append(hog_feat)\n",
        "    return np.array(feats)\n",
        "\n",
        "if train_img and test_img:\n",
        "    X_train_feat = extract_hog(X_train_img)\n",
        "    X_test_feat  = extract_hog(X_test_img)"
      ],
      "metadata": {
        "id": "B7BWZqEnM1Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 4 - Standardisasi Fitur"
      ],
      "metadata": {
        "id": "CyGLBJ1fM3yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_img and test_img:\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_feat)\n",
        "    X_test_scaled  = scaler.transform(X_test_feat)"
      ],
      "metadata": {
        "id": "KG7yA-qzM5Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 5 - Buat Data Latih dan Validasi"
      ],
      "metadata": {
        "id": "99GzmlkgM6lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_img and test_img:\n",
        "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "        X_train_scaled, y_train,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y_train # Penting untuk data imbalanced\n",
        "    )\n",
        "\n",
        "    # Convert label ke numpy array\n",
        "    y_train_split = np.array(y_train_split)\n",
        "    y_val   = np.array(y_val)\n",
        "    y_test  = np.array(y_test)"
      ],
      "metadata": {
        "id": "2nKRwt5nM7xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 6 - Buat Model ANN"
      ],
      "metadata": {
        "id": "ZdRsuCi5M9Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_img and test_img:\n",
        "    input_dim = X_train_split.shape[1]\n",
        "\n",
        "    model_daynight = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5), # Tambah dropout untuk regularisasi\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model_daynight.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model_daynight.summary()"
      ],
      "metadata": {
        "id": "PtLMMPu3M_Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 7 - Latih Model"
      ],
      "metadata": {
        "id": "Y-AW6VJ1NBeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_img and test_img:\n",
        "    history_daynight = model_daynight.fit(\n",
        "        X_train_split, y_train_split,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val, y_val)\n",
        "    )"
      ],
      "metadata": {
        "id": "jqum6WPrNC0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 8 - Evaluasi Model dengan Data Test"
      ],
      "metadata": {
        "id": "1gQannvwNEXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_img and test_img:\n",
        "    test_loss, test_acc = model_daynight.evaluate(X_test_scaled, y_test)\n",
        "    print(f\"Akurasi Test: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "uolr3dM1NLHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 9 - Buat Laporan Performansi Model"
      ],
      "metadata": {
        "id": "EmIJLqrNNMqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_img and test_img:\n",
        "    y_pred_prob = model_daynight.predict(X_test_scaled)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Night (0)', 'Day (1)']))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "vBnY_CCBNN4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 10 - Visualisasi Proses Training"
      ],
      "metadata": {
        "id": "xCmk-X-QNPL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_img and test_img:\n",
        "    history_df = pd.DataFrame(history_daynight.history)\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax[0].plot(history_df['accuracy'], label='Train Acc')\n",
        "    ax[0].plot(history_df['val_accuracy'], label='Val Acc')\n",
        "    ax[0].set_xlabel('Epoch')\n",
        "    ax[0].set_ylabel('Accuracy')\n",
        "    ax[0].set_title('Training and Validation Accuracy')\n",
        "    ax[0].legend()\n",
        "    ax[0].grid(True)\n",
        "\n",
        "    ax[1].plot(history_df['loss'], label='Train Loss')\n",
        "    ax[1].plot(history_df['val_loss'], label='Val Loss')\n",
        "    ax[1].set_xlabel('Epoch')\n",
        "    ax[1].set_ylabel('Loss')\n",
        "    ax[1].set_title('Training and Validation Loss')\n",
        "    ax[1].legend()\n",
        "    ax[1].grid(True)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BXS0BcCQNQZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 11 - Simpan Model"
      ],
      "metadata": {
        "id": "Vq4F_bR-NTZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_img and test_img:\n",
        "    model_daynight.save('day_night_classifier_model.h5')\n",
        "    print(\"Model berhasil disimpan sebagai 'day_night_classifier_model.h5'\")"
      ],
      "metadata": {
        "id": "eYtOgn0LNUwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---",
        "# ðŸŒ¹ Tugas Lab\n",
        "\n",
        "**Tugas: Lakukan klasifikasi pada data MNIST dengan menggunakan model ANN.**\n",
        "\n",
        "Anda diperbolehkan melakukan eksplorasi terhadap:\n",
        "*   Metode pra pengolahan\n",
        "*   Arsitektur ANN\n",
        "*   Fungsi Aktivasi\n",
        "\n",
        "> ANN diimplementasikan dengan menggunakan TensorFlow."
      ],
      "metadata": {
        "id": "Khb_U_vANXPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 1: Import Library dan Memuat Dataset MNIST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Memuat dataset MNIST dari TensorFlow Keras\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(f\"Ukuran data training (gambar): {x_train.shape}\")\n",
        "print(f\"Ukuran data training (label): {y_train.shape}\")\n",
        "print(f\"Ukuran data testing (gambar): {x_test.shape}\")\n",
        "print(f\"Ukuran data testing (label): {y_test.shape}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 2: Pra-Pengolahan Data (Preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi: Mengubah nilai piksel dari rentang [0, 255] ke [0, 1]\n",
        "# Ini membantu proses training menjadi lebih stabil dan cepat.\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Flattening: Mengubah gambar 28x28 piksel menjadi vektor 1D berukuran 784\n",
        "# Model Dense (fully connected) memerlukan input dalam bentuk vektor.\n",
        "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "print(f\"Ukuran data training setelah flatten: {x_train_flat.shape}\")\n",
        "print(f\"Contoh label: {y_train[0]}\")\n",
        "print(f\"Contoh data piksel (setelah normalisasi): {x_train[0][10, 10:15]}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 3: Visualisasi Contoh Data MNIST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(x_train[i], cmap='gray')\n",
        "    plt.title(f\"Label: {y_train[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"Contoh Gambar dari Dataset MNIST\")\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 4: Membangun Arsitektur Model ANN\n",
        "\n",
        "Arsitektur yang akan kita bangun:\n",
        "1.  **Input Layer**: Berupa layer `Flatten` untuk mengubah matriks 28x28 menjadi vektor 784.\n",
        "2.  **Hidden Layer 1**: `Dense` layer dengan 128 neuron dan fungsi aktivasi `ReLU`.\n",
        "3.  **Dropout Layer**: Untuk regularisasi, mencegah overfitting dengan menonaktifkan beberapa neuron secara acak selama training.\n",
        "4.  **Hidden Layer 2**: `Dense` layer dengan 64 neuron dan fungsi aktivasi `ReLU`.\n",
        "5.  **Output Layer**: `Dense` layer dengan 10 neuron (karena ada 10 kelas, 0-9) dan fungsi aktivasi `Softmax`. Softmax akan memberikan probabilitas untuk setiap kelas."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_mnist = tf.keras.models.Sequential([\n",
        "    # Tidak perlu flatten layer jika input sudah di-flatten manual\n",
        "    # tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Input(shape=(784,)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model_mnist.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy', # Cocok untuk label integer\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 5: Melatih Model (Training)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_mnist = model_mnist.fit(x_train_flat, y_train, epochs=10, validation_split=0.2)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 6: Evaluasi Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model_mnist.evaluate(x_test_flat, y_test, verbose=2)\n",
        "print(f'\\nAkurasi pada data test: {test_acc:.4f}')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 7: Visualisasi Hasil Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_df = pd.DataFrame(history_mnist.history)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "ax[0].plot(history_df['accuracy'], label='Train Acc')\n",
        "ax[0].plot(history_df['val_accuracy'], label='Val Acc')\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[0].set_title('Training and Validation Accuracy')\n",
        "ax[0].legend()\n",
        "ax[0].grid(True)\n",
        "\n",
        "ax[1].plot(history_df['loss'], label='Train Loss')\n",
        "ax[1].plot(history_df['val_loss'], label='Val Loss')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].set_title('Training and Validation Loss')\n",
        "ax[1].legend()\n",
        "ax[1].grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 8: Analisis Hasil Prediksi (Confusion Matrix dan Laporan Klasifikasi)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_prob = model_mnist.predict(x_test_flat)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1) # Mengambil kelas dengan probabilitas tertinggi\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix for MNIST Classification')\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}